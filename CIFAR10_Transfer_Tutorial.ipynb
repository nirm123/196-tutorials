{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 + Transfer Learning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "CIFAR 10 is a dataset of color images that have 10 classifications. We will be looking at different ways of creating a model for this dataset. The goals for this week are to:\n",
    "\n",
    "1. Build your own CNN for CIFAR10.\n",
    "2. Explore transfer learning and its uses (Still finishing this up).\n",
    "3. Explore the following links to think about tasks for our project moving forward. (http://cs231n.stanford.edu/reports/2016/pdfs/214_Report.pdf), (https://medium.com/@coviu/how-we-used-ai-to-translate-sign-language-in-real-time-782238ed6bf), (https://blogs.nvidia.com/blog/2017/05/11/ai-translates-sign-language/).\n",
    "\n",
    "The model for CIFAR 10 will take a much longer time to train. This is because the CIFAR10 database is much larger (more images + images are bigger) and the model is more complex. Try to budget time for running the model, but if you cannot finish training the model/the results aren't great that is okay. \n",
    "\n",
    "## Resources\n",
    "\n",
    "Really good notes if you want to get a better understanding of the topics (http://cs231n.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for building network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Libraries for dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Miscellaneous Libraries\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tutorial_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize all layers of model \"\"\"\n",
    "        # TODO                                                 \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Chain all layers together \"\"\"\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs for training\n",
    "num_epochs =      \n",
    "\n",
    "# Batch Size for training/testing\n",
    "batch_size =   \n",
    "\n",
    "# Learning Rate for optimizer\n",
    "learning_rate = \n",
    "\n",
    "# Dimensions of CIFAR10\n",
    "dim =     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation for training data\n",
    "transform_train = torchvision.transforms.Compose([\n",
    "    transforms.RandomResizedCrop(dim, scale=(0.7, 1.0), ratio=(1.0,1.0)),\n",
    "    transforms.ColorJitter(\n",
    "            brightness=0.1,\n",
    "            contrast=0.1,\n",
    "            saturation=0.1,\n",
    "            hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),                            # Convert grayscale image to pytorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,)),             # Normalize grayscale data\n",
    "])\n",
    "\n",
    "# Transformation for training data\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.CenterCrop(dim),\n",
    "    transforms.ToTensor(),                            # Convert grayscale image to pytorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,)),             # Normalize grayscale data\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data\n",
    "trainset = torchvision.datasets.CIFAR10(root='./files', train=True, download=True, \n",
    "                                      transform=transform_train)\n",
    "\n",
    "# Initialize dataloader for training data\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, \n",
    "                                           num_workers=8)\n",
    "\n",
    "# Download testing Data\n",
    "testset = torchvision.datasets.CIFAR10(root='./files', train=False, download=False, \n",
    "                                     transform=transform_test)\n",
    "\n",
    "# Initialize dataloader for testing data\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, \n",
    "                                          num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10 Dataset\n",
    "![](https://alexisbcook.github.io/assets/cifar10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model/Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize previously defined model\n",
    "model = tutorial_model()                                               \n",
    "\n",
    "# Definie loss function (Cross Entropy Loss)\n",
    "criterion = nn.CrossEntropyLoss()                                      \n",
    "\n",
    "# Initialize Optimizer (ADAM)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)     \n",
    "\n",
    "# Set model to training (updating weights)\n",
    "model.train();                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store time to calculate train time\n",
    "start_time = time.time()\n",
    "\n",
    "# Store loss and accuracy data\n",
    "loss = []\n",
    "accuracy = []\n",
    "\n",
    "# Train the model\n",
    "# Loop for number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Loop through data in batch sized increments\n",
    "    for batch_idx, (X_train_batch, Y_train_batch) in enumerate(train_loader):\n",
    "        # If trained on all data in epoch, move onto next epoch\n",
    "        if(Y_train_batch.shape[0]<batch_size):\n",
    "            continue\n",
    "\n",
    "        # Forward pass through network\n",
    "        output = model(X_train_batch)                           \n",
    "        # Calculate loss of predictions\n",
    "        curr_loss = criterion(output, Y_train_batch)            \n",
    "        # Store loss\n",
    "        loss.append(curr_loss.item())                           \n",
    "\n",
    "        # Clear last calculation\n",
    "        optimizer.zero_grad()                                   \n",
    "        # Calculate gradient based on loss\n",
    "        curr_loss.backward()                                    \n",
    "        # Update model weights\n",
    "        optimizer.step()                                        \n",
    "\n",
    "        # Extract model predictions\n",
    "        _, predicted = torch.max(output.data, 1) \n",
    "        # Calculate number of correct predictions\n",
    "        correct = (predicted == Y_train_batch).sum().item()     \n",
    "        # Calculate/store accuracy\n",
    "        accuracy.append(correct/Y_train_batch.size(0))          \n",
    "        \n",
    "        # Intermitently print statistics\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch: ' + str(epoch+1) + '/' + str(num_epochs) + ', Step: ' \n",
    "                  + str(batch_idx+1) + '/' + str(len(train_loader)) + ', Loss: ' \n",
    "                  + str(curr_loss.item()) + ', Accuracy: ' \n",
    "                  + str(correct/Y_train_batch.size(0)*100) + '%')\n",
    "\n",
    "# Store time to calculate train time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print train time\n",
    "print('Run Time: ' + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# Set model to testing (constant weights)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Store number of correct/total samples in test data\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Loop through test data\n",
    "    for X_test_batch, Y_test_batch in test_loader:\n",
    "        # Forward pass through network\n",
    "        output = model(X_test_batch)  \n",
    "        \n",
    "        # Extract prediction\n",
    "        _, predicted = torch.max(output.data, 1)    \n",
    "        \n",
    "        # Update total number of sample\n",
    "        total += Y_test_batch.size(0)  \n",
    "        \n",
    "        # Update number of correct predictions\n",
    "        correct += (predicted == Y_test_batch).sum().item()     \n",
    "\n",
    "print('Test Accuracy: ' + str((correct/total) * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
